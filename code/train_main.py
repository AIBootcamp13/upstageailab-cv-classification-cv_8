# train_seperate_aug.py
import argparse
import os
import time
import random
from datetime import datetime, timedelta, timezone

import timm
import torch
import albumentations as A
import pandas as pd
import numpy as np
import torch.nn as nn
from albumentations.pytorch import ToTensorV2
from torch.optim import Adam
import torchvision.models as models
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import StratifiedKFold
import wandb

from torch.optim.lr_scheduler import ReduceLROnPlateau
from warmup_scheduler import GradualWarmupScheduler

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class ImageDataset(Dataset):
    def __init__(self, df, train_dir, aug_dir=None, transform=None):
        self.df = df.values if isinstance(df, pd.DataFrame) else pd.read_csv(df).values
        self.train_dir = train_dir
        self.aug_dir = aug_dir
        self.transform = transform

    def __getitem__(self, idx):
        name, target = self.df[idx]
        # ì¦ê°• ì´ë¯¸ì§€ì¸ì§€ í™•ì¸
        if str(name).startswith("aug_") and self.aug_dir is not None:
            img_path = os.path.join(self.aug_dir, name)
        else:
            img_path = os.path.join(self.train_dir, name)

        img = np.array(Image.open(img_path).convert("RGB"))
        if self.transform:
            img = self.transform(image=img)["image"]
        return img, target

    def __len__(self):
        return len(self.df)

# CutMix & MixUp utils
def rand_bbox(size, lam):
    W = size[2]
    H = size[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = np.int32(W * cut_rat)
    cut_h = np.int32(H * cut_rat)
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)
    return bbx1, bby1, bbx2, bby2

def train_one_epoch(loader, model, optimizer, loss_fn, device, use_cutmix=False, use_mixup=False, alpha=1.0):
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []

    pbar = tqdm(loader)
    for image, targets in pbar:
        image = image.to(device)
        targets = targets.to(device)

        optimizer.zero_grad(set_to_none=True)

        if use_cutmix and np.random.rand() < 0.5:
            lam = np.random.beta(alpha, alpha)
            rand_index = torch.randperm(image.size()[0]).to(device)
            target_a = targets
            target_b = targets[rand_index]
            bbx1, bby1, bbx2, bby2 = rand_bbox(image.size(), lam)
            image[:, :, bbx1:bbx2, bby1:bby2] = image[rand_index, :, bbx1:bbx2, bby1:bby2]
            preds = model(image)
            loss = lam * loss_fn(preds, target_a) + (1 - lam) * loss_fn(preds, target_b)
        elif use_mixup and np.random.rand() < 0.5:
            lam = np.random.beta(alpha, alpha)
            rand_index = torch.randperm(image.size()[0]).to(device)
            mixed_image = lam * image + (1 - lam) * image[rand_index, :]
            target_a = targets
            target_b = targets[rand_index]
            preds = model(mixed_image)
            loss = lam * loss_fn(preds, target_a) + (1 - lam) * loss_fn(preds, target_b)
        else:
            preds = model(image)
            loss = loss_fn(preds, targets)

        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())

        pbar.set_description(f"Loss: {loss.item():.4f}")

    train_loss /= len(loader)
    train_acc = accuracy_score(targets_list, preds_list)
    train_f1 = f1_score(targets_list, preds_list, average='macro')

    return {"train_loss": train_loss, "train_acc": train_acc, "train_f1": train_f1}

# Validation í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate(loader, model, loss_fn, device):
    model.eval()
    val_loss = 0
    preds_list = []
    targets_list = []
    with torch.no_grad():
        for image, targets in loader:
            image = image.to(device)
            targets = targets.to(device)
            preds = model(image)
            loss = loss_fn(preds, targets)
            val_loss += loss.item()
            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
            targets_list.extend(targets.detach().cpu().numpy())
    val_loss /= len(loader)
    val_acc = accuracy_score(targets_list, preds_list)
    val_f1 = f1_score(targets_list, preds_list, average='macro')
    return {"val_loss": val_loss, "val_acc": val_acc, "val_f1": val_f1}

# í˜¼ë™ í´ë˜ìŠ¤ 3â†”7, 4â†”14ë³´ì • í•¨ìˆ˜ ì •ì˜

def correct_confused_preds(pred_df, probs):
    corrected_3_7 = 0
    corrected_4_14 = 0
    corrected_list_3_7 = []
    corrected_list_4_14 = []

    for i in range(len(pred_df)):
        pred = pred_df.loc[i, 'target']

        # 3 â†” 7 ë³´ì •
        if pred in [3, 7]:
            prob3 = probs[i][3]
            prob7 = probs[i][7]
            if abs(prob3 - prob7) < 0.05:
                new_pred = 3 if prob3 > prob7 else 7
                if new_pred != pred:
                    pred_df.loc[i, 'target'] = new_pred
                    corrected_3_7 += 1
                    corrected_list_3_7.append(pred_df.loc[i, "ID"])

        # 4 â†” 14 ë³´ì •
        elif pred in [4, 14]:
            prob4 = probs[i][4]
            prob14 = probs[i][14]
            if abs(prob4 - prob14) < 0.05:
                new_pred = 4 if prob4 > prob14 else 14
                if new_pred != pred:
                    pred_df.loc[i, 'target'] = new_pred
                    corrected_4_14 += 1
                    corrected_list_4_14.append(pred_df.loc[i, "ID"])

    print(f"ğŸ”§ [3â†”7 ë³´ì •] {corrected_3_7}ê°œ: {corrected_list_3_7}")
    print(f"ğŸ”§ [4â†”14 ë³´ì •] {corrected_4_14}ê°œ: {corrected_list_4_14}")
    print(f"âœ… ì´ ë³´ì • ì™„ë£Œ: {corrected_3_7 + corrected_4_14}ê°œ")
    return pred_df

# ====================== ë¹„ë¬¸ì„œ ì´ì§„ ë¶„ë¥˜ í›„ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ======================
def load_non_doc_classifier(device, model_name='convnext_base', binary_model_path="binary_non_doc_classifier.pth"):
    model = timm.create_model(model_name, pretrained=False, num_classes=2)
    model.load_state_dict(torch.load(binary_model_path, map_location=device))
    model.to(device)
    model.eval()
    return model
def apply_non_doc_classifier(pred_df, tst_loader, device, all_probs, args, binary_model_path="binary_non_doc_classifier.pth", model_name='convnext_base'):
    print("ğŸ“ ë¹„ë¬¸ì„œ ì´ì§„ ë¶„ë¥˜ í›„ì²˜ë¦¬ ì‹œì‘...")

    binary_model = load_non_doc_classifier(device, model_name=model_name, binary_model_path=binary_model_path)

    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])

    img_dir = os.path.join(args.data_dir, "test")
    corrected = 0
    changed_ids = []

    for i, row in pred_df.iterrows():
        img_path = os.path.join(img_dir, row['ID'])
        img = Image.open(img_path).convert("RGB")
        img = transform(img).unsqueeze(0).to(device)

        with torch.no_grad():
            pred = binary_model(img).softmax(dim=1)
            is_non_doc = pred.argmax(1).item()

        if is_non_doc:  # 2 or 16
            probs = all_probs[i]
            new_target = 2 if probs[2] > probs[16] else 16

            if pred_df.loc[i, "target"] != new_target:
                pred_df.loc[i, "target"] = new_target
                corrected += 1
                changed_ids.append(row['ID'])

    print(f"ğŸ§¹ ì‹¤ì œë¡œ target ë³´ì • ì™„ë£Œ: {corrected}ê°œ (2 or 16)")
    if changed_ids:
        print(f"ğŸ—‚ ë³€ê²½ëœ ID ëª©ë¡: {changed_ids}")
    else:
        print("âœ… ë³€ê²½ëœ ID ì—†ìŒ")
    return pred_df



# main í•¨ìˆ˜ ì •ì˜
def main():
    # argparseë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--batch_size', type=int, default=24)
    parser.add_argument('--img_size', type=int, default=300)
    parser.add_argument('--model_name', type=str, default='convnext_base')
    parser.add_argument('--exp_name', type=str, default='baseline')
    parser.add_argument('--data_dir', type=str, default='../input/data')
    parser.add_argument('--model_type', type=str, default='cnn', choices=['cnn', 'transformer'])
    parser.add_argument('--early_stop', type=int, default=5)
    parser.add_argument('--cutmix', action='store_true', help='Use CutMix augmentation')
    parser.add_argument('--mixup', action='store_true', help='Use MixUp augmentation')
    parser.add_argument('--mix_alpha', type=float, default=1.0, help='Alpha for CutMix/MixUp')
    # parser.add_argument('--label_smoothing', type=float, default=0.0, help='Label smoothing factor (0.0 to disable)')
    args = parser.parse_args()

    # WandB í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
    wandb.init(project="cnn-doc-classification", name=f"{args.model_name}_{args.exp_name}")
    wandb.config.update(args)

    # ì‹œë“œ ê³ ì •
    SEED = 42
    os.environ['PYTHONHASHSEED'] = str(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.benchmark = True

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ë°ì´í„° ë³€í™˜ ì •ì˜
    if args.model_type == 'transformer':
        trn_transform = A.Compose([
            A.Resize(height=args.img_size, width=args.img_size),

            # ğŸ“„ ë¬¸ì„œ êµ¬ì¡° ê¹¨ì§€ì§€ ì•Šê²Œ ì•½í•œ augmentation ìœ„ì£¼
            A.HorizontalFlip(p=0.4),           # ë¬¸ì„œ ì¢Œìš° ë°˜ì „: ì•½í•˜ê²Œë§Œ
            A.VerticalFlip(p=0.2),             # ìƒí•˜ ë°˜ì „ì€ ë” ì•½í•˜ê²Œ
            A.Rotate(limit=15, p=0.3),         # ì•½í•œ íšŒì „ (ë¬¸ì„œ í‹€ì–´ì§ ëŒ€ì‘)
            A.RandomRotate90(p=0.2),           # ë¹„ëŒ€ì¹­ ë¬¸ì„œ ëŒ€ì‘ìš©

            # ğŸ’¡ ìƒ‰ìƒ/ë…¸ì´ì¦ˆ ì¡°ì ˆ (TransformerëŠ” global context í•™ìŠµì´ë¯€ë¡œ ì•½í•˜ê²Œ)
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),
            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.2),
            A.GaussNoise(var_limit=(10.0, 30.0), p=0.2),

            # âœ… ê°€ì¥ ì¤‘ìš”: ê°•ì œ Resize í›„ Normalize
            A.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
        tst_transform = A.Compose([
            A.Resize(height=args.img_size, width=args.img_size),
            A.CenterCrop(height=args.img_size, width=args.img_size),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
    else:
        trn_transform = A.Compose([
            A.LongestMaxSize(max_size=args.img_size),
            A.PadIfNeeded(min_height=args.img_size, min_width=args.img_size, border_mode=0),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.7),
            A.RandomRotate90(p=1.0),
            A.Rotate(limit=30, p=0.6),
            A.GaussNoise(var_limit=(20.0, 60.0), p=0.5),
            A.MotionBlur(blur_limit=5, p=0.4),
            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.4),
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.4),
            A.RandomResizedCrop(height=args.img_size, width=args.img_size, scale=(0.6, 1.0), ratio=(0.8, 1.2), p=0.4),
            A.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
        tst_transform = A.Compose([
            A.LongestMaxSize(max_size=args.img_size),
            A.PadIfNeeded(min_height=args.img_size, min_width=args.img_size, border_mode=0),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])

    train_csv_path = os.path.join(args.data_dir, 'train.csv')
    aug_csv_path = os.path.join(args.data_dir, 'augmented.csv')
    train_img_dir = os.path.join(args.data_dir, 'train')
    aug_img_dir = os.path.join(args.data_dir, 'augmented')

    df = pd.read_csv(train_csv_path)
    aug_df = pd.read_csv(aug_csv_path)

    # ğŸ” Offline ì¦ê°•
    combined_df = pd.concat([df, aug_df], ignore_index=True) # 1.ì›ë³¸ê³¼ ì¦ê°• ë°ì´í„° ëª¨ë‘ ì‚¬ìš©
    # combined_df = aug_df # 2.ì¦ê°• ë°ì´í„°ë§Œ ì‚¬ìš©


    # 2. K-Fold split
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    for fold, (train_idx, val_idx) in enumerate(skf.split(combined_df, combined_df['target'])):
        print(f"\nğŸš€ Fold {fold} start")

        train_df = combined_df.iloc[train_idx].reset_index(drop=True)
        val_df = combined_df.iloc[val_idx].reset_index(drop=True)

        train_df_path = os.path.join(args.data_dir, 'train_split.csv')
        val_df_path = os.path.join(args.data_dir, 'val_split.csv')
        train_df.to_csv(train_df_path, index=False)
        val_df.to_csv(val_df_path, index=False)

        trn_dataset = ImageDataset(train_df, train_img_dir, aug_dir=aug_img_dir, transform=trn_transform)
        val_dataset = ImageDataset(val_df, train_img_dir, aug_dir=aug_img_dir, transform=tst_transform)
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ sample_submission.csvë¥¼ ì‚¬ìš©
        tst_dataset = ImageDataset(os.path.join(args.data_dir, 'sample_submission.csv'), os.path.join(args.data_dir, 'test'), transform=tst_transform)

        trn_loader = DataLoader(trn_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
        tst_loader = DataLoader(tst_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
        # EfficientNetV2-S: Hugging Face ê²½ë¡œë¡œ ìë™ ì¹˜í™˜
        if args.model_name == "efficientnetv2_s":
            model_name = "efficientnetv2_rw_s"
        else:
            model_name = args.model_name

        model = timm.create_model(model_name, pretrained=True, num_classes=17, img_size=(384, 384)).to(device)

        # loss_fn = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
        loss_fn = nn.CrossEntropyLoss()
        optimizer = Adam(model.parameters(), lr=args.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)

        best_val_f1 = -1.0  # <-- epoch ë£¨í”„ ì „ì— ì´ˆê¸°í™”!
        patience_counter = 0
        best_model_path = f"{args.model_name}_fold{fold}_best.pth"

        for epoch in range(args.epochs):
            metrics = train_one_epoch(
                trn_loader, model, optimizer, loss_fn, device,
                use_cutmix=args.cutmix, use_mixup=args.mixup, alpha=args.mix_alpha
            )
            val_metrics = evaluate(val_loader, model, loss_fn, device)
            metrics.update(val_metrics)
            metrics['epoch'] = epoch
            print(f"[Epoch {epoch}] Loss: {metrics['train_loss']:.4f}, Acc: {metrics['train_acc']:.4f}, F1: {metrics['train_f1']:.4f} | Val_Loss: {metrics['val_loss']:.4f}, Val_Acc: {metrics['val_acc']:.4f}, Val_F1: {metrics['val_f1']:.4f}")
            wandb.log(metrics)

            # âœ… ëª¨ë¸ ì €ì¥ ê¸°ì¤€ ë° ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ì¤€ë„ val_f1 ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
            scheduler.step(metrics['val_f1'])

            # F1 ê¸°ì¤€ìœ¼ë¡œ best model ì €ì¥

            if metrics['val_f1'] > best_val_f1 + 1e-5:  # val_f1 ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                best_val_f1 = metrics['val_f1']
                patience_counter = 0
                torch.save(model.state_dict(), best_model_path)
                print(f"\nâœ… New best model saved as {best_model_path} at epoch {epoch} with Val_F1: {best_val_f1:.4f}")
            else:
                patience_counter += 1
                print(f"\nâš ï¸ No improvement. patience_counter = {patience_counter}/{args.early_stop}")
                if patience_counter >= args.early_stop:
                    print(f"\nğŸ›‘ Early stopping triggered at epoch {epoch}.")
                    break

        # ì¶”ë¡  ìˆ˜í–‰
        print(f"\nğŸ§ª Inference using {best_model_path}")
        model.load_state_dict(torch.load(best_model_path, map_location=device))
        model.eval()

        all_probs = []
        preds_list = []
        for image, _ in tqdm(tst_loader):
            image = image.to(device)
            with torch.no_grad():
                preds = model(image)
                probs = torch.softmax(preds, dim=1).cpu().numpy()
                all_probs.extend(probs)
                preds_list.extend(np.argmax(probs, axis=1))

        pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])
        pred_df['target'] = preds_list

        # í˜¼ë™ í´ë˜ìŠ¤ ë³´ì •
        pred_df = correct_confused_preds(pred_df, all_probs)
        sample_submission_df = pd.read_csv(os.path.join(args.data_dir, 'sample_submission.csv'))
        assert (sample_submission_df['ID'] == pred_df['ID']).all()

        # ğŸ” ë¹„ë¬¸ì„œ ì´ì§„ ë¶„ë¥˜ í›„ì²˜ë¦¬ ì ìš©
        pred_df = apply_non_doc_classifier(pred_df, tst_loader, device, all_probs, args, model_name='convnext_base')

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        KST = timezone(timedelta(hours=9))
        timestamp = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), "output"))
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, f"{timestamp}_{args.model_name}_{args.exp_name}_fold{fold}.csv")
        pred_df.to_csv(filename, index=False)
        print(f"ğŸ“¦ Fold {fold} ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")

        artifact = wandb.Artifact(f"{args.exp_name}_fold{fold}", type='predictions')
        artifact.add_file(filename)
        wandb.log_artifact(artifact)

if __name__ == '__main__':
    main()
